{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ad95501-5c45-42cd-b427-e0885d0cd6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from model import Transformer, MiniLlamaArgs\n",
    "from hellaswag import render_example, iterate_examples\n",
    "from tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1e73859-c5bc-4b89-af6b-3d6f05d83325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58500/2723751875.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"fineweb_pretrain/model_19072.pt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (token_embeddings): Embedding(32000, 768)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "        (w2): Linear(in_features=2048, out_features=768, bias=False)\n",
       "        (w3): Linear(in_features=768, out_features=2048, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=768, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint = torch.load(\"log/model_19072.pt\")\n",
    "checkpoint = torch.load(\"fineweb_pretrain/model_19072.pt\")\n",
    "weights = checkpoint['model']\n",
    "\n",
    "# Init the model\n",
    "model = Transformer(MiniLlamaArgs())\n",
    "model.load_state_dict(weights)\n",
    "\n",
    "# Set Device\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# Move the model to GPU\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18f2a587-a880-4dc6-8895-b6efec821675",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9781722c-6dde-4ded-885b-5753ce92c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generator Function to generate from the model\n",
    "def generate(model, prompt):\n",
    "    enc = Tokenizer()\n",
    "    model.eval()\n",
    "    num_return_sequences = 4\n",
    "    max_length = 100\n",
    "    tokens = enc.encode(prompt, True, False)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "    tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "    xgen = tokens.to(device)\n",
    "    sample_rng = torch.Generator(device=device)\n",
    "    sample_rng.manual_seed(42)\n",
    "    \n",
    "    while xgen.size(1) < max_length:\n",
    "        # forward the model to get the logits\n",
    "        with torch.no_grad():\n",
    "            logits, loss = model(xgen) # (B, T, vocab_size)\n",
    "            # take the logits at the last position\n",
    "            logits = logits[:, -1, :] # (B, vocab_size)\n",
    "            # get the probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # do top-k sampling of 50 (huggingface pipeline default)\n",
    "            # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "            topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "            # select a token from the top-k probabilities\n",
    "            # note: multinomial does not demand the input to sum to 1\n",
    "            ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "            # gather the corresponding indices\n",
    "            xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "            # append to the sequence\n",
    "            xgen = torch.cat((xgen, xcol), dim=1)\n",
    "    \n",
    "    # print the generated text\n",
    "    for i in range(num_return_sequences):\n",
    "        tokens = xgen[i, :max_length].tolist()\n",
    "        decoded = enc.decode(tokens)\n",
    "        print(f\"Sample {i}: {decoded}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e0b1578-3a7d-4f4f-abbb-130e1a6f07e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Hello I'm an Llama and I live in a remote place (at your expense). Can you give me a summary of my current life as a Llama? I want to know, what is the importance of a living person on this planet and whether he or she is a living person?\n",
      "Do you know where your mother is or where she is born? Your mother is probably dead and therefore you have two unspoken wishes to keep her alive. How do you\n",
      "\n",
      "Sample 1: Hello I'm an Llama! The course is designed to teach parents how to interact with children at home in the best way. Through a series of simple, practical activities families can all make together to build confidence.\n",
      "Your child will develop important skills in their everyday language by having a 'home' at school. The home is where you and your child will be at different times of the week and you and your child will be in different situations. Your child's English,\n",
      "\n",
      "Sample 2: Hello I'm an Llama, and like so many who teach kids. I'm here to help you! I'm a teacher and I'm not a teacher, just a student. I love teaching, it's hard to change. What I love is the whole story of how my students got their lessons. And I really don't understand that. They're taught in the book, so I want students to grow, to know the lessons\n",
      "\n",
      "Sample 3: Hello I'm an Llama! The Crab is a crab with gills. I'm an acorn fish and I'm on a fish net. Thank you!\n",
      "Hi I'm An Llama! The Dove is a crab with gills. I'm an acorn fish, I'm on a fish net. Thank you!\n",
      "Hello I'm An Llama! The Crab is a crab with gills\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Generate from the Model I trained\n",
    "generate(model, \"Hello I'm an Llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ff9d1bb-8ff7-4727-9032-0cf629c2bb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: To stay healthy, I have to eat a whole foods diet – like chicken, eggs, grains, fruits and nuts. This includes all of the “healthy” foods, soy, dairy products, and fruits and vegetables, as well as refined grains, meats, milk, and dairy products. These foods are good. You can keep them part of a healthy diet by eating a whole food\n",
      "\n",
      "Sample 1: To stay healthy, I have to work all day. But I also require a little push back from people who have been hurt.\n",
      "How many times have you tried to help an elder baby?\n",
      "I have done so many times in my life.\n",
      "How many times have you thought you were cared for?\n",
      "An older person’s decision to stay home from work can also result in a high-risk situation called drowning. This is a serious medical emergency.\n",
      "\n",
      "Sample 2: To stay healthy, I have to be healthy!\n",
      "In this blog post, I will look at a few key health benefits of eating healthily.\n",
      "1. Eating Healthy\n",
      "Healthy foods are great for your whole body and helps fight off disease. One easy way to get healthy is by exercising and eating fruits and vegetables. Healthy fruits and vegetables are great for your brain because they can help regulate your nerv\n",
      "\n",
      "Sample 3: To stay healthy, I have to do things that help me to stay active, sleep better, and take care of myself. For me, I’ll do things that help me a lot, and for most of us, those help me to look at things differently. I’ll do things that help me to better myself, and to get more exercise.\n",
      "It really is a really important part of my life because it’s the building block of growth.\n",
      "Human beings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Generate from the Model I trained\n",
    "generate(model, \"To stay healthy, I have to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b21530-e72b-44ef-9358-79b4ad03ba0c",
   "metadata": {},
   "source": [
    "## Eval: HellaSwag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7ad9f41-0990-4e80-a50b-d9d3d5ab76ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_likely_row(tokens, mask, logits):\n",
    "    # evaluate the autoregressive loss at all positions\n",
    "    shift_logits = (logits[..., :-1, :]).contiguous()\n",
    "    shift_tokens = (tokens[..., 1:]).contiguous()\n",
    "    flat_shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "    flat_shift_tokens = shift_tokens.view(-1)\n",
    "    shift_losses = F.cross_entropy(flat_shift_logits, flat_shift_tokens, reduction='none')\n",
    "    shift_losses = shift_losses.view(tokens.size(0), -1)\n",
    "    # now get the average loss just for the completion region (where mask == 1), in each row\n",
    "    shift_mask = (mask[..., 1:]).contiguous() # we must shift mask, so we start at the last prompt token\n",
    "    masked_shift_losses = shift_losses * shift_mask\n",
    "    # sum and divide by the number of 1s in the mask\n",
    "    sum_loss = masked_shift_losses.sum(dim=1)\n",
    "    avg_loss = sum_loss / shift_mask.sum(dim=1)\n",
    "    # now we have a loss for each of the 4 completions\n",
    "    # the one with the lowest loss should be the most likely\n",
    "    pred_norm = avg_loss.argmin().item()\n",
    "    return pred_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcb73a9f-407f-4045-9789-29c068573d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: 0\n",
      "Example: 500\n",
      "Example: 1000\n",
      "Example: 1500\n",
      "Example: 2000\n",
      "Example: 2500\n",
      "Example: 3000\n",
      "Example: 3500\n",
      "Example: 4000\n",
      "Example: 4500\n",
      "Example: 5000\n",
      "Example: 5500\n",
      "Example: 6000\n",
      "Example: 6500\n",
      "Example: 7000\n",
      "Example: 7500\n",
      "Example: 8000\n",
      "Example: 8500\n",
      "Example: 9000\n",
      "Example: 9500\n",
      "Example: 10000\n",
      "HellaSwag accuracy: 3047/10042=0.3034\n"
     ]
    }
   ],
   "source": [
    "num_correct_norm = 0\n",
    "num_total = 0\n",
    "for i, example in enumerate(iterate_examples(\"val\")):\n",
    "    if i % 500 == 0:\n",
    "        print(f\"Example: {i}\")\n",
    "    # render the example into tokens and labels\n",
    "    _, tokens, mask, label = render_example(example)\n",
    "    tokens = tokens.to(device)\n",
    "    mask = mask.to(device)\n",
    "    # get the logits\n",
    "    with torch.no_grad():\n",
    "        logits, loss = model(tokens)\n",
    "        pred_norm = get_most_likely_row(tokens, mask, logits)\n",
    "    num_total += 1\n",
    "    num_correct_norm += int(pred_norm == label)\n",
    "\n",
    "acc_norm = num_correct_norm / num_total\n",
    "print(f\"HellaSwag accuracy: {num_correct_norm}/{num_total}={acc_norm:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
